{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synDataFunctionality.saveSynData import genSynDat\n",
    "import torch.utils.data as td\n",
    "from DataLoaders import SynData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test transformations work correctly on dataloader:\n",
    "from torchvision.transforms import RandomVerticalFlip, RandomHorizontalFlip, Resize, CenterCrop, Normalize\n",
    "from torchvision.transforms.functional import rotate\n",
    "import torchvision\n",
    "\n",
    "t_both = [RandomHorizontalFlip(p=0.5),  RandomVerticalFlip(p=0.5), Resize(size=(640, 640))]\n",
    "#Maybe normalize imgs automatically in dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TransDataSet = SynData(\"SynDat/synInput\", \"SynDat/synLabel\", transforms_both=t_both, repeat_channels=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test mmseg transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir checkpoints\n",
    "#!git clone https://github.com/open-mmlab/mmsegmentation.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmseg\n",
    "import mmcv\n",
    "print(mmseg.__version__)\n",
    "print(mmcv.__version__) #skal opdateres til 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmseg.apis import inference_segmentor, init_segmentor, show_result_pyplot\n",
    "from mmseg.core.evaluation import get_palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'mmsegmentation/configs/segmenter/segmenter_vit-l_mask_8x1_640x640_160k_ade20k.py'\n",
    "checkpoint_file = 'checkpoints\\segmenter_vit-l_mask_8x1_640x640_160k_ade20k_20220614_024513-4783a347.pth'\n",
    "\n",
    "config_file_segformer = 'mmsegmentation/configs/segformer/segformer_mit-b5_640x640_160k_ade20k.py'\n",
    "checkpoint_file_former = 'checkpoints\\segformer_mit-b5_640x640_160k_ade20k_20220617_203542-940a6bd8.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "#model = init_segmentor(config_file, checkpoint_file, device='cpu')\n",
    "model_segformer = init_segmentor(config_file_segformer, checkpoint_file_former, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "for param in model_segformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model_segformer.decode_head.conv_seg = torch.nn.Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
    "#model_segformer.mask_norm = torch.nn.LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
    "#model.decode_head.mask_norm = torch.nn.LayerNorm((2,), eps=1e-05, elementwise_affine=True)\n",
    "print(model_segformer)\n",
    "\n",
    "#T ODO\n",
    "#Edit channels in SegmenterMaskTransformerHead\n",
    "#Edit channels in last TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "img1, lab = TransDataSet[0]\n",
    "img1 = np.array(img1)\n",
    "img = np.rollaxis(img1,0,3)\n",
    "\n",
    "result = inference_segmentor(model_segformer, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "print(img.shape)\n",
    "plt.imshow(img[:, :, 0])\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(result[0])\n",
    "print(np.unique(result[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfa4efc2e7985e6916682396d3eaefec524ac10635f8db4811019e26e0d754ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
